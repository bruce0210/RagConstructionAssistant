# interface/pages/1_File_Upload.py
from __future__ import annotations
import os, time, json
from pathlib import Path
from typing import List, Dict, Any
import streamlit as st
import numpy as np

from core.retrieval.ingest_docx import (
    REPO_ROOT, INDEX_DIR, MEDIA_DIR,
    parse_docx_into_clauses, get_embedder, embed_texts, build_faiss_index
)
from core.utils.oss_io import get_oss_clients, oss_put

st.set_page_config(page_title="Batch DOCX Ingest to OSS", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ æ‰¹é‡ä¸Šä¼  DOCX â†’ è§£æ/å»ºç´¢å¼• â†’ åˆ†æ¡¶ä¸Š OSS")
st.caption("åŸå§‹ DOCX å­˜ A æ¡¶ï¼›æŠ½å–çš„å›¾ç‰‡å­˜ B æ¡¶ï¼›ç´¢å¼•ä¸å…ƒæ•°æ®å¯é€‰å­˜ C æ¡¶ã€‚ECS å†…ç½‘ä¸Šä¼ ï¼Œå‰ç«¯ç”¨å…¬ç½‘ URL è®¿é—®ã€‚")

# ---- å‚æ•°åŒº
with st.sidebar:
    model_name = st.selectbox(
        "Embedding æ¨¡å‹",
        ["BAAI/bge-base-zh-v1.5", "BAAI/bge-m3"],
        index=0
    )
    batch = st.number_input("Embed batch size", 8, 1024, 64, 8)
    force_cpu = st.toggle("FAISS ä»…ç”¨ CPU", value=False)
    backup_idx = st.toggle("æŠŠ faiss.index / meta.jsonl å¤‡ä»½åˆ° OSS", value=True)

# ---- è¯»å– OSS å®¢æˆ·ç«¯
try:
    secrets = st.secrets if "oss" in st.secrets else {}
    bucket_docx, bucket_media, bucket_index, url_docx, url_media, url_index = get_oss_clients(secrets)
    st.success("âœ… OSS å·²å°±ç»ªï¼ˆå†…ç½‘ä¸Šä¼  + å…¬ç½‘è®¿é—®ï¼‰ã€‚")
except Exception as e:
    st.error(f"âŒ OSS é…ç½®é”™è¯¯ï¼š{e}")
    st.stop()

# ---- æ–‡ä»¶ä¸Šä¼ 
files = st.file_uploader("é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ª DOCX", type=["docx"], accept_multiple_files=True)
if not files:
    st.info("è¯·å…ˆé€‰æ‹© DOCXã€‚")
    st.stop()

# ---- ä¸´æ—¶è½åœ°
TMP = REPO_ROOT / ".tmp_uploads"
TMP.mkdir(parents=True, exist_ok=True)
local_paths: List[Path] = []
for f in files:
    stamped = f"{int(time.time())}_{Path(f.name).name}"
    p = TMP / stamped
    with open(p, "wb") as out:
        out.write(f.read())
    local_paths.append(p)

st.write(f"ğŸ“¦ å·²æ¥æ”¶ {len(local_paths)} ä¸ªæ–‡ä»¶ã€‚")

# ---- è§£æ & æŠ½å›¾ï¼ˆå›¾ç‰‡ä¼šè½åœ¨æœ¬åœ° MEDIA_DIRï¼‰
all_clauses: List[Dict[str, Any]] = []
for i, p in enumerate(local_paths, 1):
    st.write(f"ğŸ” è§£æï¼š{p.name}")
    cs = parse_docx_into_clauses(p)
    st.write(f"ã€€â””â”€ {len(cs)} æ¡æ¬¾")
    all_clauses.extend(cs)

if not all_clauses:
    st.warning("æœªè§£æåˆ°æ¡æ¬¾ã€‚")
    st.stop()

# ---- ä¸Šä¼  DOCX åˆ° A æ¡¶ï¼ˆå¹¶ç”Ÿæˆå…¬ç½‘ URLï¼‰
st.subheader("ä¸Šä¼  DOCX åˆ° OSSï¼ˆA æ¡¶ï¼‰")
docx_url_map: Dict[str, str] = {}
for p in local_paths:
    key = f"docx/{p.name}"
    oss_put(bucket_docx, p, key)
    u = url_docx(key)
    docx_url_map[p.name] = u
    st.write(f"â˜ï¸ {p.name} â†’ {u}")

# ---- ä¸Šä¼ å›¾ç‰‡åˆ° B æ¡¶ï¼Œå¹¶æŠŠ meta é‡Œçš„ media æ”¹ä¸ºå…¬ç½‘ URL
st.subheader("ä¸Šä¼ å›¾ç‰‡åˆ° OSSï¼ˆB æ¡¶ï¼‰ï¼Œå›å¡« URL")
uploaded_media: Dict[str, str] = {}
for c in all_clauses:
    new_media = []
    for rel in c.get("media", []):
        if rel in uploaded_media:
            new_media.append(uploaded_media[rel])
            continue
        abs_path = (REPO_ROOT / rel).resolve()
        if not abs_path.exists():
            continue
        # ä»¥ MEDIA_DIR çš„ç›¸å¯¹è·¯å¾„ç»„ç»‡ key
        try:
            rel_key = abs_path.relative_to(MEDIA_DIR).as_posix()
        except Exception:
            rel_key = abs_path.name
        key = f"media/{rel_key}"
        oss_put(bucket_media, abs_path, key)
        u = url_media(key)
        uploaded_media[rel] = u
        new_media.append(u)
    c["media"] = new_media

# ---- ç”Ÿæˆå‘é‡ & æ„å»ºç´¢å¼•ï¼ˆæœ¬åœ°ï¼‰
st.subheader("Embedding & FAISS")
os.environ["RAG_EMBED_MODEL"] = model_name
model = get_embedder(model_name)
texts = [c["text"] for c in all_clauses]
vecs = embed_texts(model, texts, batch_size=int(batch))
index = build_faiss_index(vecs, use_gpu_if_possible=not force_cpu)

INDEX_DIR.mkdir(parents=True, exist_ok=True)
faiss_path = INDEX_DIR / "faiss.index"
meta_path  = INDEX_DIR / "meta.jsonl"

import faiss
faiss.write_index(index, str(faiss_path))

# metaï¼šè¡¥å…… source_urlï¼ˆDOCX çš„å…¬ç½‘ URLï¼‰
with open(meta_path, "w", encoding="utf-8") as f:
    for c in all_clauses:
        src = c.get("source", "")
        # å°è¯•åŒ¹é…æ—¶é—´æˆ³æ–‡ä»¶å
        hit = None
        for p in local_paths:
            if src == p.name or src == p.name.split("_", 1)[-1]:
                hit = docx_url_map.get(p.name)
                break
        if hit:
            c["source_url"] = hit
        f.write(json.dumps(c, ensure_ascii=False) + "\n")

st.success(f"âœ… æœ¬åœ°ç´¢å¼•ï¼š{faiss_path}")
st.success(f"âœ… æœ¬åœ°å…ƒæ•°æ®ï¼š{meta_path}")

# ---- å¯é€‰ï¼šæŠŠç´¢å¼•ä¸ meta ä¹Ÿå¤‡ä»½åˆ° OSSï¼ˆC æ¡¶ï¼‰
if backup_idx and bucket_index is not None:
    idx_key, meta_key = "index/faiss.index", "index/meta.jsonl"
    oss_put(bucket_index, faiss_path, idx_key)
    oss_put(bucket_index, meta_path,  meta_key)
    st.write(f"â˜ï¸ ç´¢å¼• â†’ {url_index(idx_key)}")
    st.write(f"â˜ï¸ å…ƒæ•°æ® â†’ {url_index(meta_key)}")

st.balloons()
st.success("ğŸ‰ å®Œæˆï¼šDOCX å…¥ A æ¡¶ã€å›¾ç‰‡å…¥ B æ¡¶ã€ç´¢å¼•å·²å»ºï¼ˆC æ¡¶å¤‡ä»½å¯é€‰ï¼‰ã€‚")
