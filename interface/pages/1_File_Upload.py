# interface/pages/1_File_Upload.py
from __future__ import annotations
import os, sys, time, json
from pathlib import Path
from typing import List, Dict, Any
from shutil import rmtree

import streamlit as st
import numpy as np
import pandas as pd  # ç”¨äºå®˜æ–¹ dataframe é¢„è§ˆ

# ---- small utils for cleanup ----
def _safe_unlink(p: Path):
    try:
        p.unlink(missing_ok=True)
    except Exception:
        pass

def _safe_rmtree(p: Path):
    try:
        rmtree(p, ignore_errors=True)
    except Exception:
        pass

def _fmt_bytes(n: int | None) -> str:
    if not isinstance(n, (int, float)) or n < 0:
        return "-"
    if n < 1024:
        return f"{n:.0f} B"
    if n < 1024**2:
        return f"{n/1024:.1f} KB"
    if n < 1024**3:
        return f"{n/1024**2:.1f} MB"
    return f"{n/1024**3:.1f} GB"
# ----------------------------------

# ---- make project root importable ---
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
# --------------------------------------

# --- fix import path so "core" can be found ---
_REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if _REPO_ROOT not in sys.path:
    sys.path.insert(0, _REPO_ROOT)
# ----------------------------------------------

# ---- import ingest module in a reload-safe way ----
import importlib
import core.retrieval.ingest_docx as ingest_docx
ingest_docx = importlib.reload(ingest_docx)  # å¼ºåˆ¶ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
# ä»æ¨¡å—å¯¹è±¡ä¸Šå–å±æ€§
REPO_ROOT  = ingest_docx.REPO_ROOT
INDEX_DIR  = ingest_docx.INDEX_DIR
MEDIA_DIR  = ingest_docx.MEDIA_DIR
parse_docx_into_clauses = ingest_docx.parse_docx_into_clauses
get_embedder            = ingest_docx.get_embedder
embed_texts             = ingest_docx.embed_texts
build_faiss_index       = ingest_docx.build_faiss_index
write_faiss_index       = ingest_docx.write_faiss_index
# è¿½åŠ æ¨¡å¼ + æ•´æ–‡æ¡£å»é‡
load_seen_doc_hashes        = ingest_docx.load_seen_doc_hashes
file_blake2b_hex            = ingest_docx.file_blake2b_hex
count_existing_meta_lines   = ingest_docx.count_existing_meta_lines
build_or_append_faiss_index = ingest_docx.build_or_append_faiss_index
write_meta_jsonl            = ingest_docx.write_meta_jsonl
# ---------------------------------------------------

from core.utils.oss_io import get_oss_clients, oss_put

# Title and welcome message
st.set_page_config(
    page_title="File Upload",
    page_icon="ğŸ“„",
    layout="centered"
)

st.title("ğŸ“„ File Upload")
st.caption("Upload construction-related documents & Batch DOCX Ingest to JSON.")

# ---- å‚æ•°åŒº
with st.sidebar:
    model_name = st.selectbox(
        "Embedding Model",
        ["BAAI/bge-base-zh-v1.5", "BAAI/bge-m3"],
        index=1  # é»˜è®¤ m3
    )
    batch = st.number_input("Embed batch size", 8, 1024, 512, 8)
    force_cpu = st.toggle("FAISS uses CPU only", value=False)
    backup_idx = st.toggle("Back up faiss.index / meta.jsonl to OSS", value=False)
    st.markdown("---")
    st.caption("ğŸ“„ Upload construction-related documents & Batch DOCX Ingest to JSON.")
    st.markdown(
        """
        <a href="https://github.com/bruce0210/rag_construction_assistant" target="_blank">
            <img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub">
        </a>
        """,
        unsafe_allow_html=True,
    )

# ---- è¯»å– OSS å®¢æˆ·ç«¯
try:
    secrets = st.secrets if "oss" in st.secrets else {}
    bucket_docx, bucket_media, bucket_index, url_docx, url_media, url_index = get_oss_clients(secrets)
    st.success("âœ…OSS is ready (intranet upload + public network access)")
except Exception as e:
    st.error(f"âŒOSS configuration errorï¼š{e}")
    st.stop()

# ---- éšè— uploader è‡ªå¸¦çš„æ–‡ä»¶å¡ç‰‡
st.markdown("""
<style>
[data-testid="stFileUploader"] [data-testid="stFileUploaderFileList"] { display:none; }
[data-testid="stFileUploader"] .uploadedFile { display:none; }
</style>
""", unsafe_allow_html=True)

# ---- æ–‡ä»¶ä¸Šä¼ ï¼ˆå¤šé€‰ï¼‰
files = st.file_uploader(
    "Please select one or more DOC / DOCX format files.",
    type=["docx"],
    accept_multiple_files=True
)
if not files:
    st.info("Please select DOC / DOCX format files first.")
    st.stop()

# å®˜æ–¹ DataFrame é¢„è§ˆ
rows = [{"File": Path(f.name).name, "Size": _fmt_bytes(getattr(f, "size", None))} for f in files]
df = pd.DataFrame(rows)
df.index = pd.RangeIndex(start=1, stop=len(df)+1, step=1, name="#")
st.dataframe(df, use_container_width=True, height=360)

start_ingest = st.button("ğŸš€ å¼€å§‹å¤„ç†å¹¶å»ºç«‹ç´¢å¼•", type="primary")
if not start_ingest:
    st.stop()

# ---- ä¸´æ—¶è½åœ°ï¼ˆä»…ç‚¹å‡»åå†™ç£ç›˜ï¼‰
TMP = REPO_ROOT / ".tmp_uploads"
TMP.mkdir(parents=True, exist_ok=True)
local_paths: List[Path] = []
for f in files:
    stamped = f"{int(time.time())}_{Path(f.name).name}"
    p = TMP / stamped
    with open(p, "wb") as out:
        out.write(f.read())
    local_paths.append(p)
st.write(f"ğŸ“¦ å·²æ¥æ”¶ {len(local_paths)} ä¸ªæ–‡ä»¶ã€‚")

# ---- ç›®æ ‡ç´¢å¼•/å…ƒæ•°æ®è·¯å¾„
INDEX_DIR.mkdir(parents=True, exist_ok=True)
faiss_path = INDEX_DIR / "faiss.index"
meta_path  = INDEX_DIR / "meta.jsonl"

# === è§£æ & æŠ½å›¾ï¼ˆè¿½åŠ æ¨¡å¼ + æ•´æ–‡æ¡£å»é‡ï¼‰ ===
parse_bar = st.progress(0, text="å‡†å¤‡è§£æâ€¦")
file_log = st.empty()
status_lines: List[str] = []

all_clauses: List[Dict[str, Any]] = []
kept_paths: List[Path] = []
seen_docs = load_seen_doc_hashes(meta_path)
batch_seen_docs: set[str] = set()

total_files = len(local_paths)
for i, p in enumerate(local_paths, 1):
    # æ•´æ–‡æ¡£æŒ‡çº¹
    try:
        doc_bytes = p.read_bytes()
    except Exception:
        doc_bytes = b""
    doc_hash = file_blake2b_hex(doc_bytes)

    # å·²å…¥åº“ / æœ¬æ‰¹é‡å¤ â†’ è·³è¿‡
    if doc_hash in seen_docs or doc_hash in batch_seen_docs:
        status_lines.append(f"{i}/{total_files} Â· {p.name} Â· å·²è·³è¿‡ï¼ˆé‡å¤æ–‡æ¡£ï¼‰")
        file_log.text("\n".join(status_lines[-30:]))
        parse_bar.progress(i/total_files, text=f"è§£æè¿›åº¦ {i}/{total_files} Â· {p.name} Â· è·³è¿‡é‡å¤")
        continue

    cs = parse_docx_into_clauses(p)
    for c in cs:
        c["doc_hash"] = doc_hash  # å†™å…¥åˆ°æ¯æ¡è®°å½•
    all_clauses.extend(cs)
    kept_paths.append(p)
    batch_seen_docs.add(doc_hash)

    status_lines.append(f"{i}/{total_files} Â· {p.name} Â· æ¡æ¬¾ {len(cs)}")
    file_log.text("\n".join(status_lines[-30:]))
    parse_bar.progress(i/total_files, text=f"è§£æè¿›åº¦ {i}/{total_files} Â· {p.name}")

if not all_clauses:
    st.info("æœ¬æ‰¹æ–‡æ¡£å‡ä¸ºé‡å¤æˆ–æœªè§£æåˆ°æœ‰æ•ˆæ¡æ¬¾ï¼Œæœªè¿›è¡Œè¿½åŠ ã€‚")
    st.stop()

# ---- ä¸Šä¼  DOCX åˆ° A æ¡¶ï¼ˆä»…æ–°æ–‡æ¡£ï¼‰
st.subheader("ä¸Šä¼  DOCX åˆ° OSSï¼ˆA æ¡¶ï¼‰")
docx_url_map: Dict[str, str] = {}
docx_bar = st.progress(0, text="å‡†å¤‡ä¸Šä¼  DOCXâ€¦")

if not kept_paths:
    st.info("æœ¬æ‰¹æ— æ–°æ–‡æ¡£éœ€è¦ä¸Šä¼ åˆ° A æ¡¶ã€‚")
else:
    for i, p in enumerate(kept_paths, 1):
        key = f"docx/{p.name}"
        oss_put(bucket_docx, p, key)
        u = url_docx(key)
        docx_url_map[p.name] = u
        st.write(f"â˜ï¸ {p.name} â†’ {u}")
        docx_bar.progress(i/len(kept_paths), text=f"DOCX ä¸Šä¼  {i}/{len(kept_paths)} Â· {p.name}")

# ---- ä¸Šä¼ å›¾ç‰‡åˆ° B æ¡¶ï¼Œå¹¶æŠŠ meta é‡Œçš„ media æ”¹ä¸ºå…¬ç½‘ URLï¼ˆä»…æ–°æ¡ç›®ï¼‰
st.subheader("ä¸Šä¼ å›¾ç‰‡åˆ° OSSï¼ˆB æ¡¶ï¼‰ï¼Œå›å¡« URL")
uploaded_media: Dict[str, str] = {}
doc_media_roots: set[Path] = set()  # æœ¬æ‰¹æ¯ä¸ª docx çš„ data/media/<slug>

total_imgs = sum(len(c.get("media", [])) for c in all_clauses)
done_imgs = 0
img_bar = st.progress(0, text="å‡†å¤‡ä¸Šä¼ å›¾ç‰‡â€¦")

for c in all_clauses:
    new_media = []
    for rel in c.get("media", []):
        # 1) åŒä¸€æ‰¹å†…é‡å¤ä½¿ç”¨åŒä¸€å¼  â†’ ç›´æ¥å¤ç”¨ URL
        if rel in uploaded_media:
            new_media.append(uploaded_media[rel])
            done_imgs += 1
            img_bar.progress(done_imgs/max(total_imgs,1), text=f"å›¾ç‰‡ä¸Šä¼  {done_imgs}/{total_imgs}")
            continue

        # 2) è®¡ç®—ç»å¯¹è·¯å¾„
        abs_path = (REPO_ROOT / rel).resolve()
        if not abs_path.exists():
            done_imgs += 1
            img_bar.progress(done_imgs/max(total_imgs,1), text=f"å›¾ç‰‡ç¼ºå¤± {done_imgs}/{total_imgs} Â· {rel}")
            continue

        # è®°å½•è¯¥æ–‡æ¡£çš„å›¾ç‰‡æ ¹ç›®å½•ï¼šdata/media/<slug>
        try:
            rel_from_media = abs_path.relative_to(MEDIA_DIR)   # "<slug>/clause_x/img_0.jpg"
            doc_root = MEDIA_DIR / rel_from_media.parts[0]      # data/media/<slug>
            doc_media_roots.add(doc_root)
        except Exception:
            pass

        # 3) ä¸Šä¼ å¹¶ç”Ÿæˆå…¬ç½‘ URL
        try:
            rel_key = abs_path.relative_to(MEDIA_DIR).as_posix()
        except Exception:
            rel_key = abs_path.name
        key = f"media/{rel_key}"
        oss_put(bucket_media, abs_path, key)
        u = url_media(key)

        uploaded_media[rel] = u
        new_media.append(u)

        done_imgs += 1
        img_bar.progress(done_imgs/max(total_imgs,1), text=f"å›¾ç‰‡ä¸Šä¼  {done_imgs}/{total_imgs} Â· {abs_path.name}")

    # â˜… ç”¨ OSS URL è¦†ç›–åŸæœ¬çš„ç›¸å¯¹è·¯å¾„
    c["media"] = new_media

# ---- ç”Ÿæˆå‘é‡ï¼ˆä»…æ–°æ¡ç›®ï¼‰ & è¿½åŠ ç´¢å¼•ï¼ˆæœ¬åœ°ï¼‰
st.subheader("Embedding & FAISS (Append Mode)")
os.environ["RAG_EMBED_MODEL"] = model_name
model = get_embedder(model_name)
texts = [c["text"] for c in all_clauses]

emb_bar = st.progress(0, text="Embeddingâ€¦")
vec_chunks = []
N = len(texts)
B = int(batch)
for j in range(0, N, B):
    sub = texts[j:j+B]
    arr = model.encode(sub, normalize_embeddings=True)
    vec_chunks.append(arr.astype("float32"))
    emb_bar.progress(min((j+len(sub))/max(N,1), 1.0), text=f"Embedding {j+len(sub)}/{N}")
vecs = np.vstack(vec_chunks)

# æ—§åº“å­˜åœ¨åˆ™åœ¨å…¶åè¿½åŠ ï¼›å¦åˆ™æ–°å»º
index = build_or_append_faiss_index(
    vecs, faiss_path, use_gpu_if_possible=not force_cpu
)
write_faiss_index(index, faiss_path)

# ---- ç”Ÿæˆ/è¿½åŠ å…ƒæ•°æ®ï¼ˆè¡¥å…… source_urlï¼‰
for c in all_clauses:
    src = c.get("source", "")
    hit = None
    for p in kept_paths:
        if src == p.name or src == p.name.split("_", 1)[-1]:
            hit = docx_url_map.get(p.name)
            break
    if hit:
        c["source_url"] = hit

base_id = count_existing_meta_lines(meta_path)
write_meta_jsonl(all_clauses, meta_path, base_id=base_id, append=True)

st.success(f"âœ… æœ¬åœ°ç´¢å¼•ï¼š{faiss_path}")
st.success(f"âœ… æœ¬åœ°å…ƒæ•°æ®ï¼š{meta_path}")

# ---- å¯é€‰ï¼šæŠŠç´¢å¼•ä¸ meta ä¹Ÿå¤‡ä»½åˆ° OSSï¼ˆC æ¡¶ï¼‰
if backup_idx and bucket_index is not None:
    idx_key, meta_key = "index/faiss.index", "index/meta.jsonl"
    oss_put(bucket_index, faiss_path, idx_key)
    oss_put(bucket_index, meta_path,  meta_key)
    st.write(f"â˜ï¸ ç´¢å¼• â†’ {url_index(idx_key)}")
    st.write(f"â˜ï¸ å…ƒæ•°æ® â†’ {url_index(meta_key)}")

# ---- ç»Ÿä¸€æ¸…ç†ï¼šä¸´æ—¶ DOCX ä¸æœ¬æ‰¹å›¾ç‰‡ç›®å½•ï¼ˆå·²å°† URL å›å¡«ä¸º OSSï¼‰
for p in local_paths:
    _safe_unlink(p)
try:
    TMP.rmdir()
except OSError:
    pass

for d in doc_media_roots:
    _safe_rmtree(d)

st.balloons()
st.success("ğŸ‰ å®Œæˆï¼šDOCX å…¥ A æ¡¶ã€å›¾ç‰‡å…¥ B æ¡¶ï¼ˆmedia å­—æ®µä¸ºå…¬ç½‘ URLï¼‰ã€ç´¢å¼•å·²è¿½åŠ ï¼ˆC æ¡¶å¤‡ä»½å¯é€‰ï¼‰ã€‚")
