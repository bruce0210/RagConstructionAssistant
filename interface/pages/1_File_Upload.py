# interface/pages/1_File_Upload.py
from __future__ import annotations
import os, sys, time, json
from pathlib import Path
from typing import List, Dict, Any
import streamlit as st
import numpy as np

# ---- make project root importable ---
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
# --------------------------------------

# --- fix import path so "core" can be found ---
_REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if _REPO_ROOT not in sys.path:
    sys.path.insert(0, _REPO_ROOT)
# ----------------------------------------------

# ---- import ingest module in a reload-safe way ----
import importlib
import core.retrieval.ingest_docx as ingest_docx
ingest_docx = importlib.reload(ingest_docx)  # å¼ºåˆ¶ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ï¼Œé¿å…æ—§ç¼“å­˜
# ä»æ¨¡å—å¯¹è±¡ä¸Šå–å±æ€§ï¼Œé¿å…â€œæŒ‰åå¯¼å…¥å‘½ä¸­æ—§ç‰ˆæœ¬â€çš„é—®é¢˜
REPO_ROOT  = ingest_docx.REPO_ROOT
INDEX_DIR  = ingest_docx.INDEX_DIR
MEDIA_DIR  = ingest_docx.MEDIA_DIR
parse_docx_into_clauses = ingest_docx.parse_docx_into_clauses
get_embedder            = ingest_docx.get_embedder
embed_texts             = ingest_docx.embed_texts
build_faiss_index       = ingest_docx.build_faiss_index  # ä»ä¿ç•™ï¼Œå¦‚éœ€æ–°å»º
write_faiss_index       = ingest_docx.write_faiss_index
# âœ… è¿½åŠ æ¨¡å¼ + æ•´æ–‡æ¡£å»é‡ï¼šæ–°å¢å·¥å…·å‡½æ•°å¼•ç”¨
load_seen_doc_hashes        = ingest_docx.load_seen_doc_hashes
file_blake2b_hex            = ingest_docx.file_blake2b_hex
count_existing_meta_lines   = ingest_docx.count_existing_meta_lines
build_or_append_faiss_index = ingest_docx.build_or_append_faiss_index
write_meta_jsonl            = ingest_docx.write_meta_jsonl
# ---------------------------------------------------

from core.utils.oss_io import get_oss_clients, oss_put

# Title and welcome message
st.set_page_config(
    page_title="File Upload",
    page_icon="ğŸ“„",
    layout="centered"
)

st.title("ğŸ“„ File Upload")
st.caption("Upload construction-related documents & Batch DOCX Ingest to JSON.")

# ---- å‚æ•°åŒº
with st.sidebar:
    model_name = st.selectbox(
        "Embedding Model",
        ["BAAI/bge-base-zh-v1.5", "BAAI/bge-m3"],
        index=1  # é»˜è®¤é€‰ m3ï¼Œå¦‚éœ€ä¿æŒåŸæ ·å¯æ”¹å› 0
    )
    batch = st.number_input("Embed batch size", 8, 1024, 64, 8)
    force_cpu = st.toggle("FAISS uses CPU only", value=False)
    backup_idx = st.toggle("Back up faiss.index / meta.jsonl to OSS", value=False)
    st.markdown("---")

    st.caption("ğŸ“„ Upload construction-related documents & Batch DOCX Ingest to JSON.")
    st.markdown(
        """
        <a href="https://github.com/bruce0210/rag_construction_assistant" target="_blank">
            <img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub">
        </a>
        """,
        unsafe_allow_html=True,
    )

# ---- è¯»å– OSS å®¢æˆ·ç«¯
try:
    secrets = st.secrets if "oss" in st.secrets else {}
    bucket_docx, bucket_media, bucket_index, url_docx, url_media, url_index = get_oss_clients(secrets)
    st.success("âœ…OSS is ready (intranet upload + public network access)")
except Exception as e:
    st.error(f"âŒOSS configuration errorï¼š{e}")
    st.stop()

# ---- æ–‡ä»¶ä¸Šä¼ 
files = st.file_uploader("Please select one or more DOC / DOCX format files.", type=["docx"], accept_multiple_files=True)
if not files:
    st.info("Please select DOC / DOCX format files first.")
    st.stop()

# ---- ä¸´æ—¶è½åœ°
TMP = REPO_ROOT / ".tmp_uploads"
TMP.mkdir(parents=True, exist_ok=True)
local_paths: List[Path] = []
for f in files:
    stamped = f"{int(time.time())}_{Path(f.name).name}"
    p = TMP / stamped
    with open(p, "wb") as out:
        out.write(f.read())
    local_paths.append(p)

st.write(f"ğŸ“¦ å·²æ¥æ”¶ {len(local_paths)} ä¸ªæ–‡ä»¶ã€‚")

# ---- ç›®æ ‡ç´¢å¼•/å…ƒæ•°æ®è·¯å¾„ï¼ˆä¾›å»é‡ä¸è¿½åŠ ä½¿ç”¨ï¼‰
INDEX_DIR.mkdir(parents=True, exist_ok=True)
faiss_path = INDEX_DIR / "faiss.index"
meta_path  = INDEX_DIR / "meta.jsonl"

# === NEW: è§£æ & æŠ½å›¾ï¼ˆè¿½åŠ æ¨¡å¼ + æ•´æ–‡æ¡£å»é‡ï¼‰ ===
parse_bar = st.progress(0, text="å‡†å¤‡è§£æâ€¦")
file_log = st.container()          # åŠ¨æ€æ»šåŠ¨æ˜¾ç¤ºâ€œå½“å‰å¤„ç†çš„æ–‡ä»¶åâ€
parsed_rows = []                   # ç´¯ç§¯æ˜¾ç¤ºæ¡ç›®

all_clauses: List[Dict[str, Any]] = []
kept_paths: List[Path] = []
seen_docs = load_seen_doc_hashes(meta_path)
batch_seen_docs: set[str] = set()

total_files = len(local_paths)
for i, p in enumerate(local_paths, 1):
    # è®¡ç®—æ•´æ–‡æ¡£æŒ‡çº¹ï¼Œç”¨äºæ•´æ–‡æ¡£å»é‡
    try:
        doc_bytes = p.read_bytes()
    except Exception:
        doc_bytes = b""
    doc_hash = file_blake2b_hex(doc_bytes)

    # é‡å¤åˆ¤å®šï¼šå·²å…¥åº“ or æœ¬æ‰¹å·²è§ â†’ è·³è¿‡
    if doc_hash in seen_docs or doc_hash in batch_seen_docs:
        parsed_rows.append(f"{i}/{total_files} Â· {p.name} Â· å·²è·³è¿‡ï¼ˆé‡å¤æ–‡æ¡£ï¼‰")
        file_log.write("\n".join(parsed_rows[-30:]))
        parse_bar.progress(i/total_files, text=f"è§£æè¿›åº¦ {i}/{total_files} Â· {p.name} Â· è·³è¿‡é‡å¤")
        continue

    cs = parse_docx_into_clauses(p)
    # ç»™æœ¬æ–‡ä»¶çš„æ‰€æœ‰æ¡ç›®æ‰“ä¸Š doc_hashï¼ˆä¾¿äºåç»­å†å»é‡ï¼‰
    for c in cs:
        c["doc_hash"] = doc_hash
    all_clauses.extend(cs)
    kept_paths.append(p)
    batch_seen_docs.add(doc_hash)

    parsed_rows.append(f"{i}/{total_files} Â· {p.name} Â· æ¡æ¬¾ {len(cs)}")
    file_log.write("\n".join(parsed_rows[-30:]))  # åªæ˜¾ç¤ºæœ€å30æ¡ï¼Œé¿å…è¿‡é•¿
    parse_bar.progress(i/total_files, text=f"è§£æè¿›åº¦ {i}/{total_files} Â· {p.name}")

if not all_clauses:
    st.info("æœ¬æ‰¹æ–‡æ¡£å‡ä¸ºé‡å¤æˆ–æœªè§£æåˆ°æœ‰æ•ˆæ¡æ¬¾ï¼Œæœªè¿›è¡Œè¿½åŠ ã€‚")
    st.stop()

# ---- ä¸Šä¼  DOCX åˆ° A æ¡¶ï¼ˆä»…ä¸Šä¼ éé‡å¤çš„ kept_pathsï¼‰
st.subheader("ä¸Šä¼  DOCX åˆ° OSSï¼ˆA æ¡¶ï¼‰")
docx_url_map: Dict[str, str] = {}
docx_bar = st.progress(0, text="å‡†å¤‡ä¸Šä¼  DOCXâ€¦")

if not kept_paths:
    st.info("æœ¬æ‰¹æ— æ–°æ–‡æ¡£éœ€è¦ä¸Šä¼ åˆ° A æ¡¶ã€‚")
else:
    for i, p in enumerate(kept_paths, 1):
        key = f"docx/{p.name}"
        oss_put(bucket_docx, p, key)
        u = url_docx(key)
        docx_url_map[p.name] = u
        st.write(f"â˜ï¸ {p.name} â†’ {u}")
        docx_bar.progress(i/len(kept_paths), text=f"DOCX ä¸Šä¼  {i}/{len(kept_paths)} Â· {p.name}")

# ---- ä¸Šä¼ å›¾ç‰‡åˆ° B æ¡¶ï¼Œå¹¶æŠŠ meta é‡Œçš„ media æ”¹ä¸ºå…¬ç½‘ URLï¼ˆä»…å¯¹æ–°æ¡ç›®ï¼‰
st.subheader("ä¸Šä¼ å›¾ç‰‡åˆ° OSSï¼ˆB æ¡¶ï¼‰ï¼Œå›å¡« URL")
uploaded_media: Dict[str, str] = {}

total_imgs = sum(len(c.get("media", [])) for c in all_clauses)
done_imgs = 0
img_bar = st.progress(0, text="å‡†å¤‡ä¸Šä¼ å›¾ç‰‡â€¦")

for c in all_clauses:
    new_media = []
    for rel in c.get("media", []):
        if rel in uploaded_media:
            new_media.append(uploaded_media[rel])
            done_imgs += 1
            img_bar.progress(done_imgs/max(total_imgs,1), text=f"å›¾ç‰‡ä¸Šä¼  {done_imgs}/{total_imgs}")
            continue

        abs_path = (REPO_ROOT / rel).resolve()
        if not abs_path.exists():
            # è®¡å…¥è¿›åº¦ï¼Œé¿å…â€œå¡ä½â€
            done_imgs += 1
            img_bar.progress(done_imgs/max(total_imgs,1), text=f"å›¾ç‰‡ç¼ºå¤± {done_imgs}/{total_imgs} Â· {rel}")
            continue

        try:
            rel_key = abs_path.relative_to(MEDIA_DIR).as_posix()
        except Exception:
            rel_key = abs_path.name
        key = f"media/{rel_key}"
        oss_put(bucket_media, abs_path, key)
        u = url_media(key)
        uploaded_media[rel] = u
        new_media.append(u)

        done_imgs += 1
        img_bar.progress(done_imgs/max(total_imgs,1), text=f"å›¾ç‰‡ä¸Šä¼  {done_imgs}/{total_imgs} Â· {abs_path.name}")

    c["media"] = new_media

# ---- ç”Ÿæˆå‘é‡ï¼ˆä»…æ–°æ¡ç›®ï¼‰ & ä»¥â€œè¿½åŠ æ¨¡å¼â€æ„å»ºç´¢å¼•ï¼ˆæœ¬åœ°ï¼‰
st.subheader("Embedding & FAISS (Append Mode)")
os.environ["RAG_EMBED_MODEL"] = model_name
model = get_embedder(model_name)
texts = [c["text"] for c in all_clauses]

# æ‰‹åŠ¨åˆ†æ‰¹åš embeddingï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
emb_bar = st.progress(0, text="Embeddingâ€¦")
vec_chunks = []
N = len(texts)
B = int(batch)
for j in range(0, N, B):
    sub = texts[j:j+B]
    arr = model.encode(sub, normalize_embeddings=True)   # ç›´æ¥è°ƒç”¨åº•å±‚ï¼Œé¿å…æ§åˆ¶å°è¿›åº¦æ¡
    vec_chunks.append(arr.astype("float32"))
    emb_bar.progress(min((j+len(sub))/max(N,1), 1.0), text=f"Embedding {j+len(sub)}/{N}")

vecs = np.vstack(vec_chunks)

# âœ… è¯»å–æ—§ç´¢å¼•å¹¶åœ¨å…¶åè¿½åŠ ï¼›è‹¥æ²¡æœ‰æ—§åº“åˆ™æ–°å»º
index = build_or_append_faiss_index(
    vecs, faiss_path, use_gpu_if_possible=not force_cpu
)
write_faiss_index(index, faiss_path)

# ---- ç”Ÿæˆ/è¿½åŠ å…ƒæ•°æ®ï¼ˆè¡¥å…… source_urlã€doc_hash å·²åœ¨è§£ææ—¶æ‰“ä¸Šï¼‰
# å…ˆæŠŠ source_url å›å¡«
for c in all_clauses:
    src = c.get("source", "")
    hit = None
    for p in kept_paths:
        if src == p.name or src == p.name.split("_", 1)[-1]:
            hit = docx_url_map.get(p.name)
            break
    if hit:
        c["source_url"] = hit

# ä»¥è¿½åŠ æ¨¡å¼å†™å…¥ meta.jsonlï¼Œå¹¶æŒ‰å·²æœ‰è¡Œæ•°ç»­å· id
base_id = count_existing_meta_lines(meta_path)
write_meta_jsonl(all_clauses, meta_path, base_id=base_id, append=True)

st.success(f"âœ… æœ¬åœ°ç´¢å¼•ï¼š{faiss_path}")
st.success(f"âœ… æœ¬åœ°å…ƒæ•°æ®ï¼š{meta_path}")

# ---- å¯é€‰ï¼šæŠŠç´¢å¼•ä¸ meta ä¹Ÿå¤‡ä»½åˆ° OSSï¼ˆC æ¡¶ï¼‰
if backup_idx and bucket_index is not None:
    idx_key, meta_key = "index/faiss.index", "index/meta.jsonl"
    oss_put(bucket_index, faiss_path, idx_key)
    oss_put(bucket_index, meta_path,  meta_key)
    st.write(f"â˜ï¸ ç´¢å¼• â†’ {url_index(idx_key)}")
    st.write(f"â˜ï¸ å…ƒæ•°æ® â†’ {url_index(meta_key)}")

st.balloons()
st.success("ğŸ‰ å®Œæˆï¼šDOCX å…¥ A æ¡¶ã€å›¾ç‰‡å…¥ B æ¡¶ã€ç´¢å¼•å·²è¿½åŠ ï¼ˆC æ¡¶å¤‡ä»½å¯é€‰ï¼‰ã€‚")
