# interface/pages/2_Prompt_Template.py
import os, json, re, textwrap, time
from datetime import datetime
import streamlit as st
import psycopg2, psycopg2.extras

# ---------------- é¡µé¢é…ç½® ----------------
st.set_page_config(page_title="Prompt Template", page_icon="ğŸ§­", layout="centered")
st.title("ğŸ§­ Prompt Template")
st.caption("Build and test customized prompt templates for construction queries.")

with st.sidebar:
    st.caption("ğŸ§­ Explore and customize prompt templates.")
    st.markdown(
        """
        <a href="https://github.com/bruce0210/rag_construction_assistant" target="_blank">
            <img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub">
        </a>
        """, unsafe_allow_html=True,
    )

# ---------------- æ•°æ®åº“ï¼ˆå«è‡ªåŠ¨è¿ç§»ï¼‰ ----------------
def get_conn():
    dsn = None
    try:
        if "postgres" in st.secrets and "dsn" in st.secrets["postgres"]:
            dsn = st.secrets["postgres"]["dsn"]
        elif "DATABASE_URL" in st.secrets:
            dsn = st.secrets["DATABASE_URL"]
    except Exception:
        pass
    if not dsn:
        dsn = os.getenv("DATABASE_URL")
    if not dsn:
        raise RuntimeError("æœªé…ç½®æ•°æ®åº“è¿æ¥ã€‚è¯·è®¾ç½® [postgres].dsn æˆ–ç¯å¢ƒå˜é‡ DATABASE_URL")
    return psycopg2.connect(dsn)

def ensure_schema():
    ddl = """
    CREATE SCHEMA IF NOT EXISTS app;

    CREATE TABLE IF NOT EXISTS app.prompt_runs (
        id               BIGSERIAL PRIMARY KEY,
        user_id          UUID,
        username         TEXT,
        template_key     TEXT NOT NULL,
        template_title   TEXT NOT NULL,
        variables        JSONB,
        input_text       TEXT,
        output_prompt    TEXT,
        polished_by_llm  BOOLEAN NOT NULL DEFAULT FALSE,
        lang             TEXT,
        created_at       TIMESTAMPTZ NOT NULL DEFAULT now(),
        page             TEXT
    );

    -- è‡ªåŠ¨è¿ç§»ï¼ˆå·²æœ‰è¡¨ä¹Ÿè¡¥é½ç¼ºåˆ—ï¼‰
    ALTER TABLE app.prompt_runs ADD COLUMN IF NOT EXISTS polished_by_llm BOOLEAN NOT NULL DEFAULT FALSE;
    ALTER TABLE app.prompt_runs ADD COLUMN IF NOT EXISTS lang TEXT;
    ALTER TABLE app.prompt_runs ADD COLUMN IF NOT EXISTS page TEXT;

    -- ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_prompt_runs_user    ON app.prompt_runs(user_id);
    CREATE INDEX IF NOT EXISTS idx_prompt_runs_created ON app.prompt_runs(created_at DESC);
    CREATE INDEX IF NOT EXISTS idx_prompt_runs_tpl     ON app.prompt_runs(template_key);
    """
    with get_conn() as conn, conn.cursor() as cur:
        cur.execute(ddl); conn.commit()

def insert_prompt_run(row: dict):
    sql = """
    INSERT INTO app.prompt_runs
      (user_id, username, template_key, template_title, variables, input_text,
       output_prompt, polished_by_llm, lang, page)
    VALUES (%s,%s,%s,%s,%s::jsonb,%s,%s,%s,%s,%s)
    RETURNING id, created_at
    """
    with get_conn() as conn, conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
        cur.execute(sql, (
            row.get("user_id"), row.get("username"),
            row.get("template_key"), row.get("template_title"),
            json.dumps(row.get("variables") or {}, ensure_ascii=False),
            row.get("input_text"), row.get("output_prompt"),
            bool(row.get("polished_by_llm", False)),
            row.get("lang") or "ZH",
            row.get("page") or "prompt_template",
        ))
        rec = cur.fetchone(); conn.commit()
        return dict(rec)

def list_recent(limit=20, kw: str | None = None, only_me: bool = False, me_id=None):
    where, args = [], []
    if kw:
        where.append("(template_title ILIKE %s OR template_key ILIKE %s OR output_prompt ILIKE %s)")
        args += [f"%{kw}%", f"%{kw}%", f"%{kw}%"]
    if only_me and me_id:
        where.append("user_id = %s"); args.append(me_id)
    sql = """
    SELECT id, created_at, username, template_title, template_key,
           LEFT(output_prompt, 180) AS preview
    FROM app.prompt_runs
    """
    if where: sql += " WHERE " + " AND ".join(where)
    sql += " ORDER BY created_at DESC LIMIT %s"; args.append(limit)
    with get_conn() as conn, conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
        cur.execute(sql, args); return cur.fetchall()

ensure_schema()

# ---------------- ç®€å•æ¨¡å¼ç®—æ³•ï¼ˆæŠŠè‡ªç„¶è¯­è¨€ â†’ æ›´å‡†çš„æ£€ç´¢çŸ­å¥ï¼‰ ----------------
STOPWORDS_ZH = set("çš„ äº† å’Œ ä¸ åŠ æˆ– åœ¨ å¯¹ äº ä¸º æ˜¯ å— å‘¢ å•Š æŠŠ è¢« å°± è¿˜ å¾ˆ æ¯” è¾ƒ æ˜¯å¦ æ€ä¹ˆ æ€ä¹ˆæ · å¦‚ä½• è¯· é—® æœ€ éœ€è¦".split())
STOPWORDS_EN = set("the a an of for to is are be in on at by with about what how why whether need minimum".split())

DISCIPLINE_HINTS = {
    "General / ç»¼åˆ": [],
    "Electrical / ç”µæ°”": ["ç…§æ˜", "é…ç”µ", "æ¥åœ°", "åº”æ€¥", "ç”µç¼†", "å˜é…ç”µ"],
    "Fire / æ¶ˆé˜²": ["ç«ç¾", "ç–æ•£", "è€ç«", "å–·æ·‹", "æŠ¥è­¦", "é˜²ç«åˆ†åŒº"],
    "HVAC / æš–é€š": ["é€šé£", "ç©ºè°ƒ", "é£é‡", "æ–°é£", "æ’çƒŸ", "çƒ­å›æ”¶"],
    "Plumbing / ç»™æ’æ°´": ["ç»™æ°´", "æ’æ°´", "è™¹å¸", "å–·æ·‹", "æ°´å‹", "æ°´æ³µ"],
}

REGION_CODES = {
    "CN (GB/JGJ/CECS)": ["GB", "JGJ", "CECS", "GB/T"],
    "EU (EN/IEC)": ["EN", "IEC"],
    "US (NFPA/ASHRAE/IPC)": ["NFPA", "ASHRAE", "IPC"],
}

def normalize_text(s: str):
    s = re.sub(r"[^\w\u4e00-\u9fa5\-\/\.#]+", " ", s, flags=re.I)
    return re.sub(r"\s+", " ", s).strip()

def keywords_from_text(s: str, lang="ZH"):
    s2 = normalize_text(s)
    toks = s2.split()
    if lang == "EN":
        toks = [t for t in toks if t.lower() not in STOPWORDS_EN]
    else:
        toks = [t for t in toks if t not in STOPWORDS_ZH and len(t) > 1]
    return toks

def build_boosted_query(user_text, discipline=None, region=None, lang="ZH", extra_constraints=""):
    toks = keywords_from_text(user_text or "", lang=lang)
    key_part = " ".join(dict.fromkeys(toks))  # å»é‡ä¿åº
    disc_part = " ".join(DISCIPLINE_HINTS.get(discipline or "", []))
    region_part = " ".join(REGION_CODES.get(region or "", []))
    cons_part = normalize_text(extra_constraints or "")
    boosted = " ".join([p for p in [key_part, disc_part, region_part, cons_part] if p])

    intent = {
        "sub_tasks": ["ç¡®å®šè§„èŒƒç±»åˆ«", "å®šä½æ¡æ¬¾å·ä¸æœ¯è¯­", "æå–é˜ˆå€¼ä¸é€‚ç”¨èŒƒå›´"],
        "keyword_queries": [
            {"task": "è§„èŒƒç±»åˆ«", "keywords": (disc_part or "").split()},
            {"task": "åŒºåŸŸæ ‡å‡†", "keywords": (region_part or "").split()},
            {"task": "é—®é¢˜è¦ç‚¹", "keywords": key_part.split()},
        ],
        "semantic_queries": [user_text.strip()[:200]],
        "fields": ["æ¡æ¬¾å·", "ç« èŠ‚", "å¹´ä»½", "å®æ–½çŠ¶æ€"],
        "rerank_rules": ["ç« èŠ‚ç²¾ç¡®åŒ¹é… > è§„èŒƒåå‘½ä¸­ > å¹´ä»½æ–°è¿‘æ€§ > è¯­ä¹‰ç›¸ä¼¼åº¦"],
        "citation_format": "ï¼ˆæ¡æ¬¾å·ï½œè§„èŒƒå…¨ç§°ï½œå¹´ä»½ï¼‰",
    }
    return boosted.strip(), intent

def maybe_refine_with_llm(draft_text: str, enable_llm: bool, lang="ZH"):
    if not enable_llm:
        return draft_text, False, None
    try:
        _ = st.secrets["openai"]["api_key"]
    except Exception:
        return draft_text, False, "No OpenAI key detected; skipped polishing."
    try:
        from openai import OpenAI
        client = OpenAI(api_key=st.secrets["openai"]["api_key"])
        sys = "You are a prompt engineer. Keep meaning, compress and clarify for retrieval. Output a single line."
        if lang == "ZH":
            sys = "ä½ æ˜¯æç¤ºè¯å·¥ç¨‹ä¸“å®¶ã€‚åœ¨ä¸æ”¹å˜æ„å›¾å‰æä¸‹å‹å®ä¸ºä¾¿äºæ£€ç´¢çš„ä¸€è¡ŒçŸ­å¥ã€‚"
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.2,
            max_tokens=200,
            messages=[{"role":"system","content":sys},{"role":"user","content":draft_text}],
        )
        out = (resp.choices[0].message.content or "").strip().replace("\n", " ")
        return out or draft_text, True, None
    except Exception as e:
        return draft_text, False, f"LLM polishing failed: {e}"

# ---------------- å•åˆ— UI ----------------
st.subheader("Language / è¯­è¨€")
lang = st.radio("", ["ä¸­æ–‡", "English"], horizontal=True, label_visibility="collapsed")
lang_code = "EN" if lang == "English" else "ZH"

st.markdown("### Simple / ç®€å•æ¨¡å¼")
user_text = st.text_area(
    "Describe your problem / æè¿°ä½ çš„é—®é¢˜",
    height=120,
    placeholder="ä¾‹ï¼šä¸€ä¸ªç…§æ˜å›è·¯ä¸­æœ€å¤šå¯ä»¥æ¥å¤šå°‘ç›ç¯ï¼Ÿ",
)
col1, col2 = st.columns(2)
discipline = col1.selectbox("Discipline / ä¸“ä¸š", list(DISCIPLINE_HINTS.keys()), index=0)
region     = col2.selectbox("Region Codes / åŒºåŸŸæ ‡å‡†", list(REGION_CODES.keys()), index=0)
constraints = st.text_input("Optional constraints / å¯é€‰çº¦æŸï¼ˆå»ºç­‘ç±»å‹ã€æˆ¿é—´ã€äººæ•°ç­‰ï¼‰", "")
use_llm = st.toggle("Use LLM to polish (optional) / ä½¿ç”¨ LLM ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰", value=False)

if st.button("ğŸš€ Optimize & Save  |  ä¼˜åŒ–å¹¶ä¿å­˜", type="primary", use_container_width=True):
    boosted, intent = build_boosted_query(user_text, discipline, region, lang_code, constraints)
    boosted2, polished, warn = maybe_refine_with_llm(boosted, use_llm, lang_code)
    if warn: st.warning(warn)
    final_query = boosted2.strip() or boosted

    st.markdown("**Optimized query / ä¼˜åŒ–æ£€ç´¢è¯­å¥**")
    st.code(final_query, language="markdown")

    user_id = st.session_state.get("user", {}).get("id")
    username = st.session_state.get("user", {}).get("username")
    payload = {
        "user_id": user_id,
        "username": username,
        "template_key": "simple_boost",
        "template_title": "æ£€ç´¢ä¼˜åŒ– / Query Boost",
        "variables": {
            "discipline": discipline, "region": region,
            "constraints": constraints, "lang": lang
        },
        "input_text": user_text,
        "output_prompt": json.dumps({"boosted_query": final_query, "intent": intent}, ensure_ascii=False),
        "polished_by_llm": polished,
        "lang": lang_code,
        "page": "prompt_template"
    }
    try:
        rec = insert_prompt_run(payload)
        st.success(f"âœ… Saved / å·²ä¿å­˜ï¼ˆID={rec['id']}ï¼‰")
        st.session_state["boosted_query"] = final_query
    except Exception as e:
        st.error(f"âŒ ä¿å­˜å¤±è´¥ï¼š{e}")

# ä¸€é”®å¸¦å» Homeï¼ˆå¦‚æœªæ‰“è¡¥ä¸ï¼Œä»å¯æ‰‹åŠ¨å¤åˆ¶ï¼‰
if st.session_state.get("boosted_query"):
    c1, c2 = st.columns([1,1])
    with c1:
        st.download_button(
            "â¬‡ï¸ Download .txt",
            data=st.session_state["boosted_query"].encode("utf-8"),
            file_name=f"query_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            use_container_width=True
        )
    with c2:
        if st.button("â¡ï¸ Use in Home  |  å¸¦å»é¦–é¡µæ£€ç´¢", use_container_width=True):
            st.session_state["home_query_prefill"] = st.session_state["boosted_query"]
            try:
                st.switch_page("Home.py")
            except Exception:
                st.markdown("[Go to Home / å‰å¾€é¦–é¡µ](../Home.py)")

# é«˜çº§æ¨¡å¼ï¼ˆæŠ˜å æ˜¾ç¤º RAG å­ä»»åŠ¡ JSONï¼‰
with st.expander("Advanced / é«˜çº§æ¨¡å¼ï¼ˆæ˜¾ç¤ºç»“æ„åŒ– RAG å­ä»»åŠ¡ï¼‰", expanded=False):
    if user_text.strip():
        tmp_boost, tmp_intent = build_boosted_query(user_text, discipline, region, lang_code, constraints)
        st.code(json.dumps(tmp_intent, ensure_ascii=False, indent=2), language="json")
    else:
        st.caption("è¯·å…ˆåœ¨ä¸Šé¢è¾“å…¥ä½ çš„é—®é¢˜ã€‚")

# å†å² / æ£€ç´¢ï¼ˆæ”¾åœ¨é¡µé¢åº•éƒ¨ï¼Œå•åˆ—æ›´æ¸…çˆ½ï¼‰
st.markdown("---")
st.markdown("### History / Search  |  å†å² / æ£€ç´¢")
# å†å² / æ£€ç´¢
me_only = st.toggle("Only mine / åªçœ‹æˆ‘çš„", value=True)
kw = st.text_input("Keyword (template/output) / å…³é”®å­—", "")
limit = st.slider("Rows / æ¡æ•°", 5, 100, 20, step=5)
me_id = st.session_state.get("user", {}).get("id")

try:
    rows = list_recent(limit=limit, kw=kw.strip() or None, only_me=me_only, me_id=me_id)
    for r in rows:
        with st.container(border=True):
            st.markdown(
                f"**#{r['id']} Â· {r['template_title']}**  \n"
                f"`{r['template_key']}` Â· by **{r['username'] or 'anonymous'}** Â· {r['created_at']:%Y-%m-%d %H:%M:%S}"
            )
            st.code((r['preview'] or '').strip(), language="markdown")
            with st.popover("View full / æŸ¥çœ‹å…¨æ–‡"):
                with get_conn() as conn, conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    cur.execute("SELECT variables, input_text, output_prompt, polished_by_llm, lang FROM app.prompt_runs WHERE id=%s", (r["id"],))
                    full = cur.fetchone()
                if full:
                    st.markdown("**Variables**"); st.code(json.dumps(full["variables"], ensure_ascii=False), language="json")
                    st.markdown("**Input**"); st.code(full["input_text"] or "", language="markdown")
                    st.markdown("**Output (JSON)**"); st.code(full["output_prompt"] or "", language="json")
                    st.caption(f"LLM polished: {'Yes' if full['polished_by_llm'] else 'No'} Â· Lang: {full.get('lang') or '-'}")

# ğŸ”§ åœ¨è¿™é‡Œè¡¥ä¸Š exceptï¼Œå’Œ try å¯¹é½
except Exception as e:
    st.error(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")

st.caption("ç™»å½•åå°†è‡ªåŠ¨æŠŠè®°å½•ä¸è´¦å·å…³è”ï¼›æœªç™»å½•ä¹Ÿå¯åŒ¿åä¿å­˜ã€‚")
