# interface/Home.py
from __future__ import annotations
import os, sys, json
from pathlib import Path
from typing import List, Dict, Any
import streamlit as st
import numpy as np

# ------------------ è½»é‡è·¯å¾„é…ç½®ï¼ˆé¿å…å¯¼å…¥é‡æ¨¡å—ï¼‰ ------------------
REPO_ROOT = Path(__file__).resolve().parents[1]
INDEX_DIR  = REPO_ROOT / "data" / "index"

# æ‡’åŠ è½½ + ç¼“å­˜ï¼šæ¨¡å‹ä¸ç´¢å¼•/å…ƒæ•°æ®ï¼Œä»…åœ¨æ£€ç´¢æ—¶è§¦å‘
@st.cache_resource(show_spinner=False)
def _load_model_cached(name: str):
    from sentence_transformers import SentenceTransformer
    device = "cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") not in (None, "", "-1") else "cpu"
    return SentenceTransformer(name, device=device)

@st.cache_data(show_spinner=False)
def _load_meta_cached(p: Path) -> List[Dict, Any]:
    if not p.exists():
        return []
    rows: List[Dict[str, Any]] = []
    with p.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                rows.append(json.loads(line))
            except Exception:
                pass
    return rows

@st.cache_resource(show_spinner=False)
def _load_faiss_cached(path: Path):
    import faiss
    if not path.exists() or path.stat().st_size == 0:
        return None
    return faiss.read_index(str(path))

def _ensure_index_ready():
    if "records" not in st.session_state:
        st.session_state.records = _load_meta_cached(INDEX_DIR / "meta.jsonl")
    if "index" not in st.session_state:
        st.session_state.index = _load_faiss_cached(INDEX_DIR / "faiss.index")
# -------------------------------------------------------------------

st.set_page_config(
    page_title="RAG Construction Assistant",
    page_icon="ğŸ—ï¸",
    layout="centered"
)

st.title("ğŸ—ï¸ğŸ” RAG Construction Assistant")
st.caption("Ask questions about building specifications, engineering standards or any construction engineering regulations.")

with st.sidebar:
    model_name = st.selectbox("Embedding Model",
                              ["BAAI/bge-base-zh-v1.5", "BAAI/bge-m3"], index=1)
    topk = st.slider("Top-K", 1, 10, 3, 1)
    st.markdown("---")
    st.caption("ğŸ—ï¸  Ask questions about building specifications, engineering standards or any construction engineering regulations.")
    st.markdown(
        """
        <a href="https://github.com/bruce0210/rag_construction_assistant" target="_blank">
            <img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub">
        </a>
        """,
        unsafe_allow_html=True,
    )

faiss_path = INDEX_DIR / "faiss.index"
meta_path  = INDEX_DIR / "meta.jsonl"

# ------------------------- UI ä¸å·¥å…·å‡½æ•° --------------------------
def _status_badge(status: str) -> str:
    if status == "ç°è¡Œ":
        color = "#16a34a"
    elif status == "åºŸæ­¢":
        color = "#B22222"
    else:
        color = "#6b7280"
    return f'<span style="background:{color};color:#fff;padding:2px 8px;border-radius:6px;font-size:12px;">{status}</span>'

def _doc_title_and_status(src: str) -> tuple[str, str]:
    name = src.split("_", 1)[-1] if "_" in src else src
    no_ext = name[:-5] if name.lower().endswith(".docx") else name
    status = no_ext[-2:] if len(no_ext) >= 2 else ""
    title = no_ext[:-2] if status in ("ç°è¡Œ","åºŸæ­¢") else no_ext
    return title, status

def search(q: str, k: int):
    _ensure_index_ready()
    records = st.session_state.get("records", [])
    index = st.session_state.get("index", None)
    if index is None or not records:
        return []

    embedder = _load_model_cached(model_name)
    vec = embedder.encode([q], normalize_embeddings=True).astype("float32")
    import faiss
    D, I = index.search(vec, k)
    out = []
    for rank, idx in enumerate(I[0], 1):
        if 0 <= idx < len(records):
            r = dict(records[idx])
            r["_score"] = float(D[0][rank-1])
            out.append(r)
    return out

# ---------------------- OpenAIï¼ˆLLM è§£è¯»ï¼Œä»…èµ°ä»£ç†ï¼‰ ----------------
def llm_answer(query: str, hits: list[dict]) -> str:
    """
    ç”¨ OpenAI åŸºäºå‘½ä¸­çš„æ¡æ¬¾ç”Ÿæˆä¸¥æ ¼â€œæœ‰æ®å¯æŸ¥â€çš„å›ç­”ã€‚
    åªå…è®¸å¼•ç”¨ hits ä¸­æä¾›çš„æ–‡æœ¬ï¼›ä¸å…è®¸è‡ªåˆ›å†…å®¹ã€‚
    """
    ctx_blocks = []
    for r in hits[:3]:
        title, status = _doc_title_and_status(r.get("source",""))
        media = r.get("media") or []
        ctx_blocks.append({
            "clause_no": r.get("clause_no",""),
            "title": title,
            "status": status,
            "text": (r.get("text") or "").strip(),
            "media": media[:5],
        })

    sys_prompt = (
        "ä½ æ˜¯å»ºç­‘å·¥ç¨‹è§„èŒƒæ£€ç´¢åŠ©æ‰‹ã€‚å¿…é¡»ä¸¥æ ¼åŸºäºæä¾›çš„â€œå‘½ä¸­æ–‡æœ¬â€å›ç­”ï¼Œ"
        "ä¸è¦ç¼–é€ è§„èŒƒæ¡æ¬¾ã€‚è¾“å‡ºç»“æ„ï¼š\n"
        "ã€ç»“è®ºã€‘ä¸€å¥è¯å›ç­”ï¼›\n"
        "ã€ä¾æ®ã€‘é€æ¡åˆ—å‡ºå¹¶æ ‡æ³¨æ¡æ¬¾å·ï¼›\n"
        "ã€æ³¨æ„äº‹é¡¹ã€‘å¦‚æœ‰åˆ™åˆ—å‡ºã€‚\n"
    )

    import httpx
    from openai import OpenAI

    def _make_http_client(proxy_url: str | None) -> httpx.Client:
        """
        å…¼å®¹ httpx 0.27 (proxies=) ä¸ 0.28+ (proxy=) çš„å†™æ³•ã€‚
        åªåœ¨ LLM è°ƒç”¨æ—¶ä½¿ç”¨ä»£ç†ã€‚
        """
        timeout = httpx.Timeout(30.0)
        if not proxy_url:
            return httpx.Client(timeout=timeout)

        # ä¼˜å…ˆå°è¯•æ—§å‚åï¼ˆ0.27 åŠä»¥ä¸‹ï¼‰
        try:
            return httpx.Client(proxies=proxy_url, timeout=timeout)
        except TypeError:
            # å›é€€åˆ°æ–°å‚åï¼ˆ0.28+ï¼‰
            return httpx.Client(proxy=proxy_url, timeout=timeout)

    # ä»£ç†é…ç½®ï¼ˆåªåœ¨ LLM è°ƒç”¨æ—¶ä½¿ç”¨ï¼‰
    proxy = st.secrets.get("openai", {}).get("proxy")  # ä¾‹å¦‚ "http://user:pass@127.0.0.1:7890"
    http_client = _make_http_client(proxy)

    client = OpenAI(
        api_key=st.secrets["openai"]["api_key"],
        http_client=http_client
    )

    payload = {"query": query, "hits": ctx_blocks}
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.2,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ],
    )
    return resp.choices[0].message.content.strip()
# -------------------------------------------------------------------

query = st.text_input("ğŸ‘·â€â™‚ï¸How can I help you with your construction project today?", "å¯¼çº¿çš„è®¾è®¡å®‰å…¨ç³»æ•°ä¸åº”å°äºå¤šå°‘?")
col_go, col_gpt = st.columns([1,1])
with col_go:
    go = st.button("ğŸš€ Go!", type="primary", use_container_width=True)
with col_gpt:
    explain_btn = st.button("ğŸ¤– LLM è§£è¯»", type="secondary", use_container_width=True)

if "show_explanation" not in st.session_state:
    st.session_state.show_explanation = False
if "explanation_text" not in st.session_state:
    st.session_state.explanation_text = ""

if go and query.strip():
    with st.spinner("Searching..."):
        hits = search(query.strip(), topk)
        st.session_state["last_hits"] = hits
    if not hits:
        st.info("å•Šå“¦...æ²¡æœ‰æ£€ç´¢åˆ°ç»“æœï¼ˆè¯·å°è¯•æ¢ä¸ªé—®æ³•~ï¼‰")
    else:
        for i, r in enumerate(hits, 1):
            with st.container(border=True):
                similarity = r.get("_score", 0.0) * 100
                st.markdown(f"**Top {i}** Â· è¯­ä¹‰æ£€ç´¢ç›¸ä¼¼åº¦={similarity:.2f}%")
                st.write((r.get("text") or "").strip())
                media = r.get("media") or []
                if isinstance(media, list) and media:
                    for url in media:
                        st.image(url, use_container_width=True)
                title, status = _doc_title_and_status(r.get("source",""))
                badge = _status_badge(status) if status else ""
                st.markdown("---")
                st.markdown(f"æœ¬æ¡æ¬¾å‡ºè‡ªè§„èŒƒï¼šã€Š{title}ã€‹", unsafe_allow_html=True)
                st.markdown(f"è¯¥è§„èŒƒå½“å‰å®æ–½çŠ¶æ€ï¼š{badge}", unsafe_allow_html=True)

if explain_btn and query.strip():
    with st.spinner("LLM æ­£åœ¨ç”Ÿæˆç»“æ„åŒ–è§£è¯»â€¦"):
        hits_for_llm = st.session_state.get("last_hits")
        if not hits_for_llm:
            hits_for_llm = search(query.strip(), topk)
            st.session_state["last_hits"] = hits_for_llm
        if hits_for_llm:
            try:
                explanation = llm_answer(query.strip(), hits_for_llm)
                st.session_state.explanation_text = explanation
                st.session_state.show_explanation = True
            except Exception as e:
                st.error(f"è°ƒç”¨ GPT å¤±è´¥ï¼š{e}")
        else:
            st.warning("æ²¡æœ‰æ£€ç´¢åˆ°æ¡æ¬¾ï¼Œæ— æ³•ç”Ÿæˆè§£è¯»ã€‚")

if st.session_state.get("show_explanation", False):
    st.markdown(
        """
        <div style="
            position: fixed; inset: 0;
            background: rgba(0,0,0,0.45);
            z-index: 9998;
        "></div>
        """,
        unsafe_allow_html=True
    )
    st.markdown(
        """
        <div style="
            position: fixed; top: 50%; left: 50%;
            transform: translate(-50%, -50%);
            background-color: #1e1e1e;
            padding: 28px;
            border-radius: 12px;
            box-shadow: 0 8px 24px rgba(0,0,0,0.7);
            z-index: 9999;
            width: min(900px, 88vw);
            max-height: 80vh; overflow-y: auto;
        ">
            <h3 style="color:#00BFFF; margin: 0 0 12px 0;">ğŸ“˜ LLM è§£è¯»</h3>
            <div style="color:white; font-size:15px; line-height:1.7;">
        """
        + st.session_state.explanation_text.replace("\n","<br/>")
        + """
            </div>
        </div>
        """,
        unsafe_allow_html=True
    )
    if st.button("âŒ å…³é—­è§£è¯»çª—å£"):
        st.session_state.show_explanation = False
